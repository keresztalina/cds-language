{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 8 - Language modelling with RNNs (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 13:09:24.789398: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-03-30 13:09:24.836591: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-03-30 13:09:24.837964: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-30 13:09:25.529380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# data processing tools\n",
    "import string, os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# keras module for building LSTM \n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import tensorflow.keras.utils as ku \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\n",
    "    action = 'ignore', \n",
    "    category = FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    # loop through string character by character. check if character is part\n",
    "    # of the string.punctuation vector and if yes then remove it. convert \n",
    "    # everything else to lowercase. concatenate to form new string\n",
    "    # resulting txt string will contain the original string with all \n",
    "    # punctuation marks removed and all characters converted to lowercase\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    # convert text to bytes with UTF-8 encoding, then take resulting output\n",
    "    # and translate it back into strings while keeping all ASCII-characters\n",
    "    # and removing non-ASCII characters. great if original string contains \n",
    "    # characters that cannot be processed by certain systems or software\n",
    "    txt = txt.encode(\"utf8\").decode(\n",
    "        \"ascii\",\n",
    "        'ignore')\n",
    "    return txt \n",
    "\n",
    "def get_sequence_of_tokens(tokenizer, corpus):\n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        # convert each line into a sequence of integers\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        # for each token, find the token that comes after it\n",
    "        # add to list\n",
    "        # (thereby you have a nice list with input words and word that can\n",
    "        # come after them)\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    # get the length of the longest sequence\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    # make every sequence the length of the longest on\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, \n",
    "                                            maxlen=max_sequence_len, \n",
    "                                            padding='pre'))\n",
    "\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, \n",
    "                            num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "def create_model(max_sequence_len, total_words):\n",
    "    # max_sequence_len: an integer representing the maximum length of a text sequence\n",
    "    # total_words: an integer representing the total number of unique words in the \n",
    "    # vocabulary of the text corpus\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential() # model takes one word after the other sequentially\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    # creates dense vector representation for each input word\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            total_words, # the total number of unique words in the vocabulary\n",
    "            10, # the size of the vector space in which the words will be embedded\n",
    "            input_length = input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(\n",
    "        LSTM(\n",
    "            100)) # number of memory cells in the layer\n",
    "    # prevent overfitting by randomly dropping out some of the connections between \n",
    "    # the LSTM cells\n",
    "    model.add(\n",
    "        Dropout(\n",
    "            0.1)) # dropout rate\n",
    "    \n",
    "    # Add Output Layer\n",
    "    # generate a probability distribution over the vocabulary of possible next words \n",
    "    # in the sequence\n",
    "    model.add(\n",
    "        Dense(\n",
    "            total_words, # the total number of unique words in the vocabulary\n",
    "            activation = 'softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss = 'categorical_crossentropy',\n",
    "        optimizer = 'adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    # seed_text: a string representing the starting text for text generation\n",
    "    # next_words: an integer representing the number of words to generate after \n",
    "    # the seed text\n",
    "    # model: a trained Keras neural network model that will be used for text \n",
    "    # generation\n",
    "    # max_sequence_len: an integer representing the maximum length of the input \n",
    "    # sequence that the model was trained on\n",
    "    for _ in range(next_words):\n",
    "        # convert seed_text to tokens\n",
    "        token_list = tokenizer.texts_to_sequences([\n",
    "            seed_text])[0]\n",
    "        # pad the sequence with zeros to match the length of the input sequences \n",
    "        # that the model was trained on\n",
    "        token_list = pad_sequences([\n",
    "            token_list],\n",
    "            maxlen = max_sequence_len-1,\n",
    "            padding = 'pre') # add zeros before the sequence\n",
    "        # determine the index of the word with the highest predicted probability \n",
    "        # in the output vocabulary\n",
    "        predicted = np.argmax(\n",
    "            model.predict(\n",
    "                token_list),\n",
    "                axis = 1)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        # look up the actual word corresponding to the predicted index in the \n",
    "        # tokenizer's word index, then appends this word to the seed_text variable\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\n",
    "    \"..\", \n",
    "    \"..\", \n",
    "    \"..\", \n",
    "    \"431868\", \n",
    "    \"news_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def1347c459f24986d7c80</td>\n",
       "      <td>716</td>\n",
       "      <td>By STEPHEN HILTNER and SUSAN LEHMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>Finding an Expansive View  of a Forgotten Peop...</td>\n",
       "      <td>['Photography', 'New York Times', 'Niger', 'Fe...</td>\n",
       "      <td>3</td>\n",
       "      <td>Insider</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-04-01 00:15:41</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>One of the largest photo displays in Times his...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/insider/nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def3237c459f24986d7c84</td>\n",
       "      <td>823</td>\n",
       "      <td>By GAIL COLLINS</td>\n",
       "      <td>article</td>\n",
       "      <td>And Now,  the Dreaded Trump Curse</td>\n",
       "      <td>['United States Politics and Government', 'Tru...</td>\n",
       "      <td>3</td>\n",
       "      <td>OpEd</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-04-01 00:23:58</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Meet the gang from under the bus.</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Op-Ed</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/opinion/and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def9f57c459f24986d7c90</td>\n",
       "      <td>575</td>\n",
       "      <td>By THE EDITORIAL BOARD</td>\n",
       "      <td>article</td>\n",
       "      <td>Venezuela’s Descent Into Dictatorship</td>\n",
       "      <td>['Venezuela', 'Politics and Government', 'Madu...</td>\n",
       "      <td>3</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-04-01 00:53:06</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>A court ruling annulling the legislature’s aut...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Editorial</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/opinion/ven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58defd317c459f24986d7c95</td>\n",
       "      <td>1374</td>\n",
       "      <td>By MICHAEL POWELL</td>\n",
       "      <td>article</td>\n",
       "      <td>Stain Permeates Basketball Blue Blood</td>\n",
       "      <td>['Basketball (College)', 'University of North ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Sports</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-01 01:06:52</td>\n",
       "      <td>College Basketball</td>\n",
       "      <td>For two decades, until 2013, North Carolina en...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/sports/ncaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58df09b77c459f24986d7ca7</td>\n",
       "      <td>708</td>\n",
       "      <td>By DEB AMLEN</td>\n",
       "      <td>article</td>\n",
       "      <td>Taking Things for Granted</td>\n",
       "      <td>['Crossword Puzzles']</td>\n",
       "      <td>3</td>\n",
       "      <td>Games</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-01 02:00:14</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>In which Howard Barkin and Will Shortz teach u...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/crosswords/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58fd41ab7c459f24986dbaa7</td>\n",
       "      <td>710</td>\n",
       "      <td>By ANDREW E. KRAMER</td>\n",
       "      <td>article</td>\n",
       "      <td>Reporting on Gays Who ‘Don’t Exist’</td>\n",
       "      <td>['Chechnya (Russia)', 'Homosexuality and Bisex...</td>\n",
       "      <td>3</td>\n",
       "      <td>Insider</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-04-24 00:07:04</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>“I see flies, I see mosquitoes,” said a Cheche...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/04/23/insider/rus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58fd45a17c459f24986dbaaa</td>\n",
       "      <td>1230</td>\n",
       "      <td>By MATT FLEGENHEIMER and THOMAS KAPLAN</td>\n",
       "      <td>article</td>\n",
       "      <td>The Fights That Could Lead to a Government Shu...</td>\n",
       "      <td>['Trump, Donald J', 'United States Politics an...</td>\n",
       "      <td>3</td>\n",
       "      <td>National</td>\n",
       "      <td>15</td>\n",
       "      <td>2017-04-24 00:23:53</td>\n",
       "      <td>Politics</td>\n",
       "      <td>The Trump administration wants to use the dead...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/04/23/us/politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58fd5c2c7c459f24986dbac3</td>\n",
       "      <td>1424</td>\n",
       "      <td>By NOEL MURRAY</td>\n",
       "      <td>article</td>\n",
       "      <td>‘The Leftovers’ Season 3, Episode 2: Swedish P...</td>\n",
       "      <td>['Television', 'The Leftovers (TV Program)']</td>\n",
       "      <td>3</td>\n",
       "      <td>Culture</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-24 02:00:04</td>\n",
       "      <td>Television</td>\n",
       "      <td>For all its melancholy, “The Leftovers” rarely...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Review</td>\n",
       "      <td>https://www.nytimes.com/2017/04/23/arts/televi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58fd5c3d7c459f24986dbac4</td>\n",
       "      <td>1052</td>\n",
       "      <td>By BEN BRANTLEY</td>\n",
       "      <td>article</td>\n",
       "      <td>Thinking Out Loud, But Why?</td>\n",
       "      <td>['Theater', 'The Antipodes (Play)', 'Baker, An...</td>\n",
       "      <td>3</td>\n",
       "      <td>Culture</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-24 02:00:25</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>In this endlessly fascinating work, Annie Bake...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Review</td>\n",
       "      <td>https://www.nytimes.com/2017/04/23/theater/the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58fd5c3d7c459f24986dbac5</td>\n",
       "      <td>981</td>\n",
       "      <td>By BEN BRANTLEY</td>\n",
       "      <td>article</td>\n",
       "      <td>Some Sugar. Could Use More Spice.</td>\n",
       "      <td>['Theater', 'Charlie and the Chocolate Factory...</td>\n",
       "      <td>3</td>\n",
       "      <td>Culture</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-04-24 02:00:26</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Christian Borle is the eccentric Willy Wonka i...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Review</td>\n",
       "      <td>https://www.nytimes.com/2017/04/23/theater/cha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>886 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    abstract                 articleID  articleWordCount  \\\n",
       "0        NaN  58def1347c459f24986d7c80               716   \n",
       "1        NaN  58def3237c459f24986d7c84               823   \n",
       "2        NaN  58def9f57c459f24986d7c90               575   \n",
       "3        NaN  58defd317c459f24986d7c95              1374   \n",
       "4        NaN  58df09b77c459f24986d7ca7               708   \n",
       "..       ...                       ...               ...   \n",
       "881      NaN  58fd41ab7c459f24986dbaa7               710   \n",
       "882      NaN  58fd45a17c459f24986dbaaa              1230   \n",
       "883      NaN  58fd5c2c7c459f24986dbac3              1424   \n",
       "884      NaN  58fd5c3d7c459f24986dbac4              1052   \n",
       "885      NaN  58fd5c3d7c459f24986dbac5               981   \n",
       "\n",
       "                                     byline documentType  \\\n",
       "0       By STEPHEN HILTNER and SUSAN LEHMAN      article   \n",
       "1                           By GAIL COLLINS      article   \n",
       "2                    By THE EDITORIAL BOARD      article   \n",
       "3                         By MICHAEL POWELL      article   \n",
       "4                              By DEB AMLEN      article   \n",
       "..                                      ...          ...   \n",
       "881                     By ANDREW E. KRAMER      article   \n",
       "882  By MATT FLEGENHEIMER and THOMAS KAPLAN      article   \n",
       "883                          By NOEL MURRAY      article   \n",
       "884                         By BEN BRANTLEY      article   \n",
       "885                         By BEN BRANTLEY      article   \n",
       "\n",
       "                                              headline  \\\n",
       "0    Finding an Expansive View  of a Forgotten Peop...   \n",
       "1                    And Now,  the Dreaded Trump Curse   \n",
       "2                Venezuela’s Descent Into Dictatorship   \n",
       "3                Stain Permeates Basketball Blue Blood   \n",
       "4                            Taking Things for Granted   \n",
       "..                                                 ...   \n",
       "881                Reporting on Gays Who ‘Don’t Exist’   \n",
       "882  The Fights That Could Lead to a Government Shu...   \n",
       "883  ‘The Leftovers’ Season 3, Episode 2: Swedish P...   \n",
       "884                        Thinking Out Loud, But Why?   \n",
       "885                  Some Sugar. Could Use More Spice.   \n",
       "\n",
       "                                              keywords  multimedia    newDesk  \\\n",
       "0    ['Photography', 'New York Times', 'Niger', 'Fe...           3    Insider   \n",
       "1    ['United States Politics and Government', 'Tru...           3       OpEd   \n",
       "2    ['Venezuela', 'Politics and Government', 'Madu...           3  Editorial   \n",
       "3    ['Basketball (College)', 'University of North ...           3     Sports   \n",
       "4                                ['Crossword Puzzles']           3      Games   \n",
       "..                                                 ...         ...        ...   \n",
       "881  ['Chechnya (Russia)', 'Homosexuality and Bisex...           3    Insider   \n",
       "882  ['Trump, Donald J', 'United States Politics an...           3   National   \n",
       "883       ['Television', 'The Leftovers (TV Program)']           3    Culture   \n",
       "884  ['Theater', 'The Antipodes (Play)', 'Baker, An...           3    Culture   \n",
       "885  ['Theater', 'Charlie and the Chocolate Factory...           3    Culture   \n",
       "\n",
       "     printPage              pubDate         sectionName  \\\n",
       "0            2  2017-04-01 00:15:41             Unknown   \n",
       "1           23  2017-04-01 00:23:58             Unknown   \n",
       "2           22  2017-04-01 00:53:06             Unknown   \n",
       "3            1  2017-04-01 01:06:52  College Basketball   \n",
       "4            0  2017-04-01 02:00:14             Unknown   \n",
       "..         ...                  ...                 ...   \n",
       "881          2  2017-04-24 00:07:04             Unknown   \n",
       "882         15  2017-04-24 00:23:53            Politics   \n",
       "883          0  2017-04-24 02:00:04          Television   \n",
       "884          1  2017-04-24 02:00:25             Unknown   \n",
       "885          2  2017-04-24 02:00:26             Unknown   \n",
       "\n",
       "                                               snippet              source  \\\n",
       "0    One of the largest photo displays in Times his...  The New York Times   \n",
       "1                    Meet the gang from under the bus.  The New York Times   \n",
       "2    A court ruling annulling the legislature’s aut...  The New York Times   \n",
       "3    For two decades, until 2013, North Carolina en...  The New York Times   \n",
       "4    In which Howard Barkin and Will Shortz teach u...  The New York Times   \n",
       "..                                                 ...                 ...   \n",
       "881  “I see flies, I see mosquitoes,” said a Cheche...  The New York Times   \n",
       "882  The Trump administration wants to use the dead...  The New York Times   \n",
       "883  For all its melancholy, “The Leftovers” rarely...  The New York Times   \n",
       "884  In this endlessly fascinating work, Annie Bake...  The New York Times   \n",
       "885  Christian Borle is the eccentric Willy Wonka i...  The New York Times   \n",
       "\n",
       "    typeOfMaterial                                             webURL  \n",
       "0             News  https://www.nytimes.com/2017/03/31/insider/nig...  \n",
       "1            Op-Ed  https://www.nytimes.com/2017/03/31/opinion/and...  \n",
       "2        Editorial  https://www.nytimes.com/2017/03/31/opinion/ven...  \n",
       "3             News  https://www.nytimes.com/2017/03/31/sports/ncaa...  \n",
       "4             News  https://www.nytimes.com/2017/03/31/crosswords/...  \n",
       "..             ...                                                ...  \n",
       "881           News  https://www.nytimes.com/2017/04/23/insider/rus...  \n",
       "882           News  https://www.nytimes.com/2017/04/23/us/politics...  \n",
       "883         Review  https://www.nytimes.com/2017/04/23/arts/televi...  \n",
       "884         Review  https://www.nytimes.com/2017/04/23/theater/the...  \n",
       "885         Review  https://www.nytimes.com/2017/04/23/theater/cha...  \n",
       "\n",
       "[886 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(data_dir + \"/\" + \"ArticlesApril2017.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're then going to load the data one at a time and append *only* the headlines to our list of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headlines = []\n",
    "for filename in os.listdir(data_dir):\n",
    "    if 'Articles' in filename:\n",
    "        article_df = pd.read_csv(data_dir + \"/\" + filename)\n",
    "        all_headlines.extend(list(article_df[\"headline\"].values))#keep headline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then clean up a little bit and see how many data points we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8603"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove \"Unknown\" headlines\n",
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "len(all_headlines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call out ```clean_text()``` function and then inspect the first 10 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "corpus[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "We're then going to tokenize our data, using the ```Tokenizer()``` class from ```TensorFlow```, about which you can read more [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer).\n",
    "\n",
    "We then use the ```get_sequence_of_tokens()``` function we defined above, which turns every text into a sequence of tokens based on the vocabulary from the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "## tokenization\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_sequences, total_words = get_sequence_of_tokens(tokenizer, corpus)\n",
    "inp_sequences[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to *pad* our input sequences to make them all the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the ```create_model()``` function created above to initialize a model, telling the model the length of sequences and the total size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training is exactly the same as last week, but instead of document labels, we're fitting the model to predict next word.\n",
    "\n",
    "*NB!* This will take some time to train! It took me 35 minutes on UCloud 32xCPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(predictors, \n",
    "                    label, \n",
    "                    epochs=100,\n",
    "                    batch_size=128, \n",
    "                    verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model has trained, we can then use this to generate *new text*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"danish\", 5, model, max_sequence_len))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having the embedding layer as a trainable parameter, we can instead using a *pretrained word embedding* model like ```word2vec```.\n",
    "\n",
    "In the following examples, we're using [GloVe embeddings](https://nlp.stanford.edu/projects/glove/). These are trained a little differently from ```word2vec``` but they behave in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = os.path.join(\"path/to/glove/vectors\")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define some variables that we're going to use later.\n",
    "\n",
    "With hits and misses, we're counting how many words in the corpus vocabulary have a corresponding GloVe embedding; misses are the words which appear in our vocabulary but which do not have a GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = total_words\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer - notice that this is different\n",
    "    model.add(Embedding(\n",
    "            total_words,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "            trainable=False,\n",
    "            input_length=input_len)\n",
    "    )\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(500))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, \n",
    "                    activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                    optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(predictors, \n",
    "                    label, \n",
    "                    epochs=100,\n",
    "                    batch_size=128, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"china\", 30, model, max_sequence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (default, Feb 28 2021, 17:03:44) \n[GCC 10.2.1 20210110]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
