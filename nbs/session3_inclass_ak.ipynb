{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ```spaCy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of different NLP frameworks that you're likely to encounter. The most popular and widely-used of these are:\n",
    "\n",
    "- ```NLTK``` (Natural Language Toolkit, old-school)\n",
    "- ```UDPipe``` (Neural network based, fast and light, but not super accurate)\n",
    "- ```CoreNLP``` and ```stanza``` (Created by the team at Stanford; academically robust)\n",
    "- ```spaCy``` production-ready, well-documented, state-of-the-art\n",
    "\n",
    "We'll be working with ```spaCy``` in this module, primarily because it's easy and intuitive, and also scales well.\n",
    "\n",
    "First thing we need to do is install ```spaCy``` and the language model that we want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing ```spaCy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is import ```spaCy``` __and__ the language model that we want to use.\n",
    "\n",
    "Note that, if you want to use different langauges you want to use different language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spacy NLP object\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model now loaded, we can begin to do some very simple NLP tasks.\n",
    "\n",
    "Here, we create a spaCy object and assign it to the variable ```nlp```. This is the NLP pipeline that will do all our heavy lifting, using the trained model we've specified.\n",
    "\n",
    "Below, you can see what the pipeline does with a bit of sample text. Passing text to the nlp object gives us access to a bunch of properties, including tokens (words), parts of speech, named entities, and so on. Here's we two of them, tokens and entities. These objects, in turn, have certain methods attached to them. A full outline of available methods can be found in the spaCy docs.\n",
    "\n",
    "In this case, for all token objects, let's return the token itself (token.text); its part-of-speech tag (token.pos_); and the grammatical dependency relations between the tokens (token.dep_).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"My name is Alina and I was born in Hungary. I currently live in Aarhus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Alina and I was born in Hungary. I currently live in Aarhus.\n"
     ]
    }
   ],
   "source": [
    "print(doc) #data type is no longer a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenize__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My\n",
      "name\n",
      "is\n",
      "Alina\n",
      "and\n",
      "I\n",
      "was\n",
      "born\n",
      "in\n",
      "Hungary\n",
      ".\n",
      "I\n",
      "currently\n",
      "live\n",
      "in\n",
      "Aarhus\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Trying some more attributes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 My my PRON poss Number=Sing|Person=1|Poss=Yes|PronType=Prs\n",
      "1 name name NOUN nsubj Number=Sing\n",
      "2 is be AUX ROOT Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "3 Alina Alina PROPN attr Number=Sing\n",
      "4 and and CCONJ cc ConjType=Cmp\n",
      "5 I I PRON nsubjpass Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "6 was be AUX auxpass Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "7 born bear VERB conj Aspect=Perf|Tense=Past|VerbForm=Part\n",
      "8 in in ADP prep \n",
      "9 Hungary Hungary PROPN pobj Number=Sing\n",
      "10 . . PUNCT punct PunctType=Peri\n",
      "11 I I PRON nsubj Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "12 currently currently ADV advmod \n",
      "13 live live VERB ROOT Tense=Pres|VerbForm=Fin\n",
      "14 in in ADP prep \n",
      "15 Aarhus Aarhus PROPN pobj Number=Sing\n",
      "16 . . PUNCT punct PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.i, token.text, token.lemma_, token.pos_, token.dep_, token.morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token.pos = part of speech tagging. We get a number which corresponds to a part of speech (just represented as numbers). If we run it with an underscore token.pos_ then we get the name of the part of speech. \n",
    "\n",
    "token.morph = more finegrained linguistic information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NER__\n",
    "\n",
    "Extracting named entities from a spaCy doc requires an extra step, but nothing too challenging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alina PERSON\n",
      "Hungary GPE\n",
      "Aarhus GPE\n"
     ]
    }
   ],
   "source": [
    "# extracting NERs\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "# Alina is a person, Hungary and Aarhus are both geopolitical entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count distribution of linguistic features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create doc object__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file name\n",
    "import os \n",
    "filename = os.path.join(\"..\", \"data\", \"example.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text file\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as file: \n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "# count adjectives\n",
    "adjective_count = 0\n",
    "for token in doc:\n",
    "    if token.pos_ == \"ADJ\":\n",
    "        adjective_count += 1\n",
    "print(adjective_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list\n",
    "entities = []\n",
    "\n",
    "# get named entities\n",
    "for ent in doc.ents: \n",
    "    entities.append(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PPE', 'Jorge', 'Israel', '18 October 2020', 'UK', 'Taylor Swift', 'Instagram', 'YouTube', 'Hanan', 'Team Jorge', 'ICO', 'first', 'one', 'Twitter', 'Amazon', 'Sheffield', 'Facebook', 'Gmail', 'Team Jorge’s Aims', 'Canaelan', 'Guardian', '2,000', 'multimillion-pound', 'Advanced Impact Media Solutions', 'Tottenham Hotspur', 'Tal Hanan', 'Twitter two days later', 'LinkedIn', 'Telegram', 'Der Spiegel', 'Airbnb', 'Information Commissioner’s Office', 'Le Monde', 'more than 30,000'}\n"
     ]
    }
   ],
   "source": [
    "print(set(entities)) # all unique objects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relative frequency__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the relative frequency per 10,000 words\n",
    "# if our word doc was 10000 long, how many ADJ would we expect to see?\n",
    "relative_freq = adjective_count/len(doc)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text has a relative frequency of 72 adjectives per 1000 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"This text has a relative frequency of {int(relative_freq)} adjectives per 1000 tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating neater outputs using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, all of our output from ```spaCy``` is in the form of lists. If we want to save these, it probably makes sense to have them saved in a more transferable format, such as CSV files or JSONs.\n",
    "\n",
    "One very easy way to do this with Python is by using the dataframe library ```pandas```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spaCy doc\n",
    "# a single sentence example\n",
    "input_string = \"My name is Alina and I was born in Hungary.\"\n",
    "\n",
    "# create a new Doc object (spacy DOCs are complex objects)\n",
    "doc = nlp(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "\n",
    "for token in doc: \n",
    "    annotations.append((token.text, token.pos_)) # append two things by making it into a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy doc to pandas dataframe\n",
    "data = pd.DataFrame(annotations, columns=[\"tokens\", \"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>name</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alina</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>was</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>born</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hungary</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tokens    pos\n",
       "0        My   PRON\n",
       "1      name   NOUN\n",
       "2        is    AUX\n",
       "3     Alina  PROPN\n",
       "4       and  CCONJ\n",
       "5         I   PRON\n",
       "6       was    AUX\n",
       "7      born   VERB\n",
       "8        in    ADP\n",
       "9   Hungary  PROPN\n",
       "10        .  PUNCT"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "outpath = os.path.join(\"..\", \"out\", \"annotations.csv\")\n",
    "data.to_csv(outpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
